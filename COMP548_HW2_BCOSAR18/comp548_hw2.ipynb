{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ICdtHWszHNzo",
    "outputId": "7ea4f769-9105-477d-efb2-b9c547c6a4a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "UWssZhmdHnKo"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader,Dataset, WeightedRandomSampler\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import random_split\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "mYa_e7NxIJqe"
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_weights_for_balanced_classes(images, nclasses):\n",
    "    count = [0] * nclasses\n",
    "    for item in images:\n",
    "        count[item[1]] += 1\n",
    "    weight_per_class = [0.] * nclasses\n",
    "    N = float(sum(count))\n",
    "    for i in range(nclasses):\n",
    "        weight_per_class[i] = N/float(count[i])\n",
    "    weight = [0] * len(images)\n",
    "    for idx, val in enumerate(images):\n",
    "        weight[idx] = weight_per_class[val[1]]\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "y3454T3YJrqM"
   },
   "outputs": [],
   "source": [
    "data_dir=\"/content/drive/MyDrive/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "a96_syTTILnE"
   },
   "outputs": [],
   "source": [
    "image_folder_train = os.path.join(data_dir,\"md_hw2/dataset/training\")\n",
    "image_folder_test = os.path.join(data_dir,\"md_hw2/dataset/test\")\n",
    "image_paths_train =[os.path.join(image_folder_train,filename) for filename in os.listdir(image_folder_train) if filename.endswith(\".jpg\")]\n",
    "image_paths_test= [os.path.join(image_folder_test, filename) for filename in os.listdir(image_folder_test) if filename.endswith(\".jpg\")]\n",
    "\n",
    "labels_file_train = os.path.join(data_dir, \"md_hw2/dataset/training_labels.txt\")\n",
    "labels_file_test= os.path.join(data_dir, \"md_hw2/dataset/test_labels.txt\")\n",
    "labels_df_train = pd.read_csv(labels_file_train, delimiter=\" \", header=None, names=[\"Label\"])\n",
    "labels_df_test = pd.read_csv(labels_file_test, delimiter=\" \", header=None, names=[\"Label\"])\n",
    "labels_df_train[\"Label\"] -= 1\n",
    "labels_df_test[\"Label\"] -= 1\n",
    "\n",
    "train_indices = range(0, len(image_paths_train))\n",
    "test_indices = range(0, len(image_paths_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "nhTMG1lsKHCF"
   },
   "outputs": [],
   "source": [
    "train_image_paths = [image_paths_train[i] for i in train_indices]\n",
    "train_labels = [labels_df_train.loc[i, \"Label\"] for i in train_indices]\n",
    "\n",
    "test_image_paths = [image_paths_test[i] for i in test_indices]\n",
    "test_labels = [labels_df_test.loc[i, \"Label\"] for i in test_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "raiAwHqKKKVV"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "JDPuFmC7KNbv"
   },
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    " #   transforms.RandomResizedCrop(224),\n",
    "#    transforms.RandomHorizontalFlip(),\n",
    " #   transforms.RandomRotation(30),\n",
    "    transforms.ToTensor(),\n",
    "   # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    transforms.Normalize([0.7108, 0.6784, 0.7242], [0.1603, 0.1857, 0.1047])\n",
    "])\n",
    "\n",
    "transform_test=transforms.Compose([\n",
    "   transforms.Resize(224),\n",
    "  # transforms.CenterCrop((224,224)),\n",
    "   transforms.ToTensor(),\n",
    " #  transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "   transforms.Normalize([0.7117, 0.6809, 0.7257], [0.1587, 0.1835, 0.1033])\n",
    "\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "wHSdVgChKQGr"
   },
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_image_paths, train_labels, transform=transform)\n",
    "test_dataset = CustomDataset(test_image_paths, test_labels, transform=transform_test)\n",
    "#weights = make_weights_for_balanced_classes(train_dataset,3)\n",
    "#sampler = WeightedRandomSampler(weights, len(train_dataset), replacement=True)\n",
    "\n",
    "train_dataset, val_dataset = random_split(train_dataset, [len(train_dataset) - 38, 38])\n",
    "\n",
    "weights = make_weights_for_balanced_classes(train_dataset,3)\n",
    "sampler = WeightedRandomSampler(weights, len(train_dataset), replacement=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, sampler=sampler)\n",
    "#train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "val_loader=torch.utils.data.DataLoader(val_dataset,batch_size=4,shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tVSDktBSKYZT",
    "outputId": "dd8fdd9a-6343-4588-ef7f-683b240f67cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model=torchvision.models.alexnet(pretrained=True)\n",
    "\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad=False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YHqDBhvL6-pu",
    "outputId": "bae68727-5703-4c3f-f8ad-026d0fb926ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "CQDC7x_yMYkd"
   },
   "outputs": [],
   "source": [
    "\n",
    "num_ftrs = model.classifier[6].in_features\n",
    "model.classifier[6] = nn.Linear(num_ftrs, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VpgGcun-7Kds",
    "outputId": "34548955-22a7-4a78-912b-0bf6c0bf28d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "Bh6hMXj_MgW4"
   },
   "outputs": [],
   "source": [
    "\n",
    "criterion=torch.nn.CrossEntropyLoss()\n",
    "#criterion=torch.nn.functional.cross_entropy\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=model.to(device)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DlZ-x6kmM05Y",
    "outputId": "03c425c8-a1c6-4b79-83a7-52ccbad554dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Loss: 0.4632, Train Accuracy: 0.8716\n",
      "Epoch [1/20], Validation Accuracy: 0.9737\n",
      "Epoch [2/20], Train Loss: 0.1903, Train Accuracy: 0.9459\n",
      "Epoch [2/20], Validation Accuracy: 0.9737\n",
      "Epoch [3/20], Train Loss: 0.2461, Train Accuracy: 0.9257\n",
      "Epoch [3/20], Validation Accuracy: 0.6842\n",
      "Epoch [4/20], Train Loss: 0.3482, Train Accuracy: 0.9122\n",
      "Epoch [4/20], Validation Accuracy: 0.8421\n",
      "Epoch [5/20], Train Loss: 0.0993, Train Accuracy: 0.9527\n",
      "Epoch [5/20], Validation Accuracy: 0.9474\n",
      "Epoch [6/20], Train Loss: 0.0959, Train Accuracy: 0.9730\n",
      "Epoch [6/20], Validation Accuracy: 0.9474\n",
      "Epoch [7/20], Train Loss: 0.0779, Train Accuracy: 0.9865\n",
      "Epoch [7/20], Validation Accuracy: 0.9474\n",
      "Epoch [8/20], Train Loss: 0.0178, Train Accuracy: 1.0000\n",
      "Epoch [8/20], Validation Accuracy: 0.9474\n",
      "Epoch [9/20], Train Loss: 0.0496, Train Accuracy: 0.9730\n",
      "Epoch [9/20], Validation Accuracy: 0.9474\n",
      "Epoch [10/20], Train Loss: 0.0462, Train Accuracy: 0.9865\n",
      "Epoch [10/20], Validation Accuracy: 0.9474\n",
      "Epoch [11/20], Train Loss: 0.0358, Train Accuracy: 0.9797\n",
      "Epoch [11/20], Validation Accuracy: 0.9474\n",
      "Epoch [12/20], Train Loss: 0.0254, Train Accuracy: 1.0000\n",
      "Epoch [12/20], Validation Accuracy: 0.9474\n",
      "Epoch [13/20], Train Loss: 0.0218, Train Accuracy: 0.9932\n",
      "Epoch [13/20], Validation Accuracy: 0.9474\n",
      "Epoch [14/20], Train Loss: 0.0400, Train Accuracy: 0.9797\n",
      "Epoch [14/20], Validation Accuracy: 0.9474\n",
      "Epoch [15/20], Train Loss: 0.0302, Train Accuracy: 0.9932\n",
      "Epoch [15/20], Validation Accuracy: 0.9474\n",
      "Epoch [16/20], Train Loss: 0.0293, Train Accuracy: 0.9932\n",
      "Epoch [16/20], Validation Accuracy: 0.9474\n",
      "Epoch [17/20], Train Loss: 0.0287, Train Accuracy: 0.9932\n",
      "Epoch [17/20], Validation Accuracy: 0.9474\n",
      "Epoch [18/20], Train Loss: 0.0203, Train Accuracy: 1.0000\n",
      "Epoch [18/20], Validation Accuracy: 0.9474\n",
      "Epoch [19/20], Train Loss: 0.0301, Train Accuracy: 0.9865\n",
      "Epoch [19/20], Validation Accuracy: 0.9474\n",
      "Epoch [20/20], Train Loss: 0.0514, Train Accuracy: 0.9865\n",
      "Epoch [20/20], Validation Accuracy: 0.9474\n",
      "Training finished.\n",
      "133\n",
      "144\n",
      "Test Accuracy: 0.9236%\n",
      "Correct: 133\n",
      "Total: 144\n",
      "Test Accuracy: 0.9236%\n",
      "Precision: 0.9814814814814815\n",
      "Recall: 0.9636363636363636\n",
      "F1-score: 0.9724770642201834\n",
      "Class Accuracies - Training:\n",
      "Class 0 Accuracy: 1.0000%\n",
      "Class 1 Accuracy: 0.9773%\n",
      "Class 2 Accuracy: 0.9833%\n",
      "Class Accuracies - Validation:\n",
      "Class 0 Accuracy: 1.0000%\n",
      "Class 1 Accuracy: 0.9048%\n",
      "Class 2 Accuracy: 1.0000%\n",
      "Class Accuracies - Test:\n",
      "Class 0 Accuracy: 0.9792%\n",
      "Class 1 Accuracy: 0.9298%\n",
      "Class 2 Accuracy: 0.8462%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    class_correct_train = [0] * 3\n",
    "    class_total_train = [0] * 3\n",
    "#    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "#        class_weights=class_weights.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        for label, prediction in zip(labels, predicted):\n",
    "            if label == prediction:\n",
    "                class_correct_train[label] += 1\n",
    "            class_total_train[label] += 1\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "           # print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "           print('Epoch [{}/{}], Batch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
    "    scheduler.step()\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    train_acc = correct / total\n",
    "#    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Train Accuracy: {train_acc:.4f}')\n",
    "    print('Epoch [{}/{}], Train Loss: {:.4f}, Train Accuracy: {:.4f}'.format(epoch+1, num_epochs, epoch_loss, train_acc))\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    class_correct_val = [0] * 3\n",
    "    class_total_val = [0] * 3\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs,labels) in enumerate(val_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            for label, prediction in zip(labels, predicted):\n",
    "                if label == prediction:\n",
    "                    class_correct_val[label] += 1\n",
    "                class_total_val[label] += 1\n",
    "\n",
    "    val_acc = correct / total\n",
    "    print('Epoch [{}/{}], Validation Accuracy: {:.4f}'.format(epoch+1, num_epochs, val_acc))\n",
    "    scheduler.step()\n",
    "print('Training finished.')\n",
    "\n",
    "correct=0\n",
    "total=0\n",
    "true_positives = 0\n",
    "false_positives = 0\n",
    "false_negatives = 0\n",
    "class_correct = [0] * 3\n",
    "class_total = [0] * 3\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (inputs,labels) in enumerate(test_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Get predictions\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "\n",
    "        # Update counts\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        for label, prediction in zip(labels, predicted):\n",
    "            if label == prediction:\n",
    "                class_correct[label] += 1\n",
    "            class_total[label] += 1\n",
    "\n",
    "        true_positives += ((predicted == 1) & (labels == 1)).sum().item()\n",
    "        false_positives += ((predicted == 1) & (labels == 0)).sum().item()\n",
    "        false_negatives += ((predicted == 0) & (labels == 1)).sum().item()\n",
    "accuracy = correct / total\n",
    "print(correct)\n",
    "print(total)\n",
    "print('Test Accuracy: {:.4f}%'.format(accuracy))\n",
    "\n",
    "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "\n",
    "# Calculate Recall\n",
    "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "\n",
    "# Calculate F1-score\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(\"Correct:\", correct)\n",
    "print(\"Total:\", total)\n",
    "print('Test Accuracy: {:.4f}%'.format(accuracy))\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1_score)\n",
    "\n",
    "print(\"Class Accuracies - Training:\")\n",
    "for i in range(3):\n",
    "        class_accuracy_train = class_correct_train[i] / class_total_train[i] if class_total_train[i] > 0 else 0\n",
    "        print('Class {} Accuracy: {:.4f}%'.format(i, class_accuracy_train))\n",
    "\n",
    "print(\"Class Accuracies - Validation:\")\n",
    "for i in range(3):\n",
    "        class_accuracy_val = class_correct_val[i] / class_total_val[i] if class_total_val[i] > 0 else 0\n",
    "        print('Class {} Accuracy: {:.4f}%'.format(i, class_accuracy_val))\n",
    "\n",
    "print(\"Class Accuracies - Test:\")\n",
    "for i in range(3):\n",
    "    class_accuracy = class_correct[i] / class_total[i] if class_total[i] > 0 else 0\n",
    "    print('Class {} Accuracy: {:.4f}%'.format(i, class_accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
